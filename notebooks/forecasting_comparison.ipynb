{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b3f7b6",
   "metadata": {},
   "source": [
    "# PHASE 2 - FORECASTING ABONNEMENTS FIBRE\n",
    "## Comparaison de Mod√®les de Pr√©vision Temporelle\n",
    "\n",
    "**Objectif :** Comparer 5 mod√®les de pr√©vision (Prophet, SARIMA, XGBoost, LSTM, Exponential Smoothing) pour pr√©dire les abonnements quotidiens fibre sur 30-90 jours.\n",
    "\n",
    "**Donn√©es :** Fibre Forecast Database (PostgreSQL)\n",
    "- P√©riode : 01/2024 ‚Üí 12/2025 (730 jours)\n",
    "- Cible : Nombre d'abonnements par jour\n",
    "- M√©trique succ√®s : MAPE < 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdd1ee",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries & Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e113208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Machine Learning & Time Series\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Time Series Models\n",
    "from prophet import Prophet\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "print(\"‚úì Toutes les librairies import√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc899c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics functions\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Root Mean Squared Error\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate all metrics at once\"\"\"\n",
    "    return {\n",
    "        'MAE': mae(y_true, y_pred),\n",
    "        'RMSE': rmse(y_true, y_pred),\n",
    "        'MAPE': mape(y_true, y_pred),\n",
    "        'sMAPE': smape(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "print(\"‚úì M√©triques d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5666de5",
   "metadata": {},
   "source": [
    "## Section 2: Load Data from PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b74388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "DB_USER = os.getenv('POSTGRES_USER', 'admin')\n",
    "DB_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'SecurePassword123!')\n",
    "DB_HOST = os.getenv('POSTGRES_HOST', 'localhost')\n",
    "DB_PORT = os.getenv('POSTGRES_PORT', '5432')\n",
    "DB_NAME = os.getenv('POSTGRES_DB', 'fibre_forecast_db')\n",
    "\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "print(f\"üîó Connexion √† la base de donn√©es : {DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "# Load daily aggregated data\n",
    "query_daily = \"\"\"\n",
    "    SELECT \n",
    "        DATE(f.created_at) as date,\n",
    "        COUNT(*) as nb_abonnements\n",
    "    FROM mart.fact_abonnements f\n",
    "    GROUP BY DATE(f.created_at)\n",
    "    ORDER BY date;\n",
    "\"\"\"\n",
    "\n",
    "df_daily = pd.read_sql(query_daily, engine)\n",
    "df_daily['date'] = pd.to_datetime(df_daily['date'])\n",
    "df_daily = df_daily.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Load detailed data for feature engineering\n",
    "query_detailed = \"\"\"\n",
    "    SELECT \n",
    "        DATE(f.created_at) as date,\n",
    "        t.jour_semaine,\n",
    "        t.mois,\n",
    "        t.trimestre,\n",
    "        t.est_weekend,\n",
    "        t.est_ferie,\n",
    "        g.governorate,\n",
    "        o.categorie as offre_categorie,\n",
    "        o.debit\n",
    "    FROM mart.fact_abonnements f\n",
    "    JOIN mart.dim_temps t ON f.date_id = t.date_id\n",
    "    JOIN mart.dim_geographie g ON f.geo_id = g.geo_id\n",
    "    JOIN mart.dim_offres o ON f.offre_id = o.offre_id\n",
    "    ORDER BY date;\n",
    "\"\"\"\n",
    "\n",
    "df_detailed = pd.read_sql(query_detailed, engine)\n",
    "df_detailed['date'] = pd.to_datetime(df_detailed['date'])\n",
    "\n",
    "print(f\"‚úì Donn√©es charg√©es : {len(df_daily)} jours\")\n",
    "print(f\"  P√©riode : {df_daily['date'].min().date()} ‚Üí {df_daily['date'].max().date()}\")\n",
    "print(f\"  Total abonnements : {df_daily['nb_abonnements'].sum()}\")\n",
    "print(f\"  Moyenne/jour : {df_daily['nb_abonnements'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af789bed",
   "metadata": {},
   "source": [
    "## Section 3: Exploratory Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05988c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time series\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Full time series\n",
    "axes[0].plot(df_daily['date'], df_daily['nb_abonnements'], linewidth=1.5, color='steelblue')\n",
    "axes[0].fill_between(df_daily['date'], df_daily['nb_abonnements'], alpha=0.3, color='steelblue')\n",
    "axes[0].set_title('S√©rie Temporelle Compl√®te - Abonnements Fibre Quotidiens', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Nombre d\\'abonnements')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Statistics by day of week\n",
    "df_daily['jour_semaine'] = df_daily['date'].dt.dayofweek\n",
    "df_daily['sem_num'] = df_daily['date'].dt.isocalendar().week\n",
    "\n",
    "daily_by_dow = df_daily.groupby('jour_semaine')['nb_abonnements'].mean()\n",
    "dow_names = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']\n",
    "axes[1].bar(range(7), daily_by_dow.values, color='coral', alpha=0.7)\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(dow_names)\n",
    "axes[1].set_title('Moyenne d\\'abonnements par Jour de la Semaine', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Nombre d\\'abonnements')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Rolling mean\n",
    "df_daily['rolling_7'] = df_daily['nb_abonnements'].rolling(window=7).mean()\n",
    "df_daily['rolling_30'] = df_daily['nb_abonnements'].rolling(window=30).mean()\n",
    "axes[2].plot(df_daily['date'], df_daily['nb_abonnements'], label='Quotidien', linewidth=0.8, alpha=0.5, color='gray')\n",
    "axes[2].plot(df_daily['date'], df_daily['rolling_7'], label='Moyenne mobile 7j', linewidth=2, color='orange')\n",
    "axes[2].plot(df_daily['date'], df_daily['rolling_30'], label='Moyenne mobile 30j', linewidth=2, color='red')\n",
    "axes[2].set_title('Tendance avec Moyennes Mobiles', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Nombre d\\'abonnements')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "plot_acf(df_daily['nb_abonnements'].dropna(), lags=40, ax=axes[0])\n",
    "plot_pacf(df_daily['nb_abonnements'].dropna(), lags=40, ax=axes[1])\n",
    "axes[0].set_title('ACF - Autocorrelation Function')\n",
    "axes[1].set_title('PACF - Partial Autocorrelation Function')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"\\nüìä Statistiques Descriptives:\")\n",
    "print(df_daily['nb_abonnements'].describe())\n",
    "print(f\"\\nSkewness : {df_daily['nb_abonnements'].skew():.4f}\")\n",
    "print(f\"Kurtosis : {df_daily['nb_abonnements'].kurtosis():.4f}\")\n",
    "\n",
    "# Check for missing dates\n",
    "expected_days = (df_daily['date'].max() - df_daily['date'].min()).days + 1\n",
    "missing_days = expected_days - len(df_daily)\n",
    "print(f\"\\n‚ö†Ô∏è  Jours manquants : {missing_days} / {expected_days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25272af5",
   "metadata": {},
   "source": [
    "## Section 4: Feature Engineering for ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature engineering dataset\n",
    "df_features = df_daily.copy()\n",
    "\n",
    "# Temporal features\n",
    "df_features['jour_mois'] = df_features['date'].dt.day\n",
    "df_features['mois'] = df_features['date'].dt.month\n",
    "df_features['trimestre'] = df_features['date'].dt.quarter\n",
    "df_features['semaine_annee'] = df_features['date'].dt.isocalendar().week\n",
    "df_features['est_weekend'] = df_features['jour_semaine'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Cyclical features\n",
    "df_features['jour_annee'] = df_features['date'].dt.dayofyear\n",
    "df_features['jour_annee_sin'] = np.sin(2 * np.pi * df_features['jour_annee'] / 365.25)\n",
    "df_features['jour_annee_cos'] = np.cos(2 * np.pi * df_features['jour_annee'] / 365.25)\n",
    "\n",
    "# Lag features (J-1, J-7, J-14, J-30)\n",
    "for lag in [1, 7, 14, 30]:\n",
    "    df_features[f'lag_{lag}'] = df_features['nb_abonnements'].shift(lag)\n",
    "\n",
    "# Rolling mean features (7, 14, 30 days)\n",
    "for window in [7, 14, 30]:\n",
    "    df_features[f'rolling_mean_{window}'] = df_features['nb_abonnements'].rolling(window=window).mean()\n",
    "    df_features[f'rolling_std_{window}'] = df_features['nb_abonnements'].rolling(window=window).std()\n",
    "\n",
    "# Tunisian holidays\n",
    "tunisian_holidays = pd.to_datetime([\n",
    "    '2024-01-01', '2024-03-20', '2024-04-09', '2024-05-01',\n",
    "    '2024-07-25', '2024-08-13', '2024-10-15',\n",
    "    '2025-01-01', '2025-03-20', '2025-05-01',\n",
    "    '2025-07-25', '2025-08-13', '2025-10-15'\n",
    "])\n",
    "df_features['est_ferie'] = df_features['date'].isin(tunisian_holidays).astype(int)\n",
    "\n",
    "# Top 3 governorates and offers from detailed data\n",
    "daily_gov = df_detailed.groupby(['date', 'governorate']).size().reset_index(name='count')\n",
    "daily_offre = df_detailed.groupby(['date', 'offre_categorie']).size().reset_index(name='count')\n",
    "\n",
    "top_govs = df_detailed['governorate'].value_counts().head(3).index.tolist()\n",
    "top_offres = df_detailed['offre_categorie'].value_counts().head(3).index.tolist()\n",
    "\n",
    "for gov in top_govs:\n",
    "    gov_data = daily_gov[daily_gov['governorate'] == gov].groupby('date')['count'].sum()\n",
    "    df_features = df_features.merge(\n",
    "        gov_data.rename(f'gov_{gov}').to_frame().reset_index(),\n",
    "        left_on='date', right_on='date', how='left'\n",
    "    )\n",
    "    df_features[f'gov_{gov}'] = df_features[f'gov_{gov}'].fillna(0)\n",
    "\n",
    "for offre in top_offres:\n",
    "    offre_data = daily_offre[daily_offre['offre_categorie'] == offre].groupby('date')['count'].sum()\n",
    "    df_features = df_features.merge(\n",
    "        offre_data.rename(f'offre_{offre}').to_frame().reset_index(),\n",
    "        left_on='date', right_on='date', how='left'\n",
    "    )\n",
    "    df_features[f'offre_{offre}'] = df_features[f'offre_{offre}'].fillna(0)\n",
    "\n",
    "# Drop rows with NaN from lags\n",
    "df_features_clean = df_features.dropna()\n",
    "\n",
    "print(f\"‚úì Features cr√©√©es : {df_features_clean.shape[1] - 1} features\")\n",
    "print(f\"‚úì Lignes valides : {len(df_features_clean)} (apr√®s suppression NaN)\")\n",
    "print(f\"\\nFeatures disponibles :\")\n",
    "feature_cols = [col for col in df_features_clean.columns if col not in ['date', 'nb_abonnements']]\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532787bd",
   "metadata": {},
   "source": [
    "## Section 5: Train/Test Split (Temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cad972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal train/test split (80/20)\n",
    "train_size = int(len(df_features_clean) * 0.8)\n",
    "\n",
    "df_train = df_features_clean.iloc[:train_size].copy()\n",
    "df_test = df_features_clean.iloc[train_size:].copy()\n",
    "\n",
    "# For time series models (Prophet, SARIMA, etc.)\n",
    "train_ts = df_daily.iloc[:train_size].copy()\n",
    "test_ts = df_daily.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"üìä Train/Test Split (Temporal):\")\n",
    "print(f\"  Train : {df_train['date'].min().date()} ‚Üí {df_train['date'].max().date()} ({len(df_train)} jours)\")\n",
    "print(f\"  Test  : {df_test['date'].min().date()} ‚Üí {df_test['date'].max().date()} ({len(df_test)} jours)\")\n",
    "print(f\"  Ratio : {len(df_train)/(len(df_train)+len(df_test))*100:.1f}% / {len(df_test)/(len(df_train)+len(df_test))*100:.1f}%\")\n",
    "\n",
    "# Prepare feature matrices for ML models\n",
    "feature_cols = [col for col in df_features_clean.columns if col not in ['date', 'nb_abonnements']]\n",
    "\n",
    "X_train = df_train[feature_cols].values\n",
    "y_train = df_train['nb_abonnements'].values\n",
    "X_test = df_test[feature_cols].values\n",
    "y_test = df_test['nb_abonnements'].values\n",
    "\n",
    "# For simple models (baseline)\n",
    "y_train_full_ts = df_daily.iloc[:train_size]['nb_abonnements'].values\n",
    "y_test_full_ts = df_daily.iloc[train_size:]['nb_abonnements'].values\n",
    "\n",
    "print(f\"\\n‚úì Features pr√©par√©es : X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "\n",
    "# Visualize train/test split\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(df_train['date'], df_train['nb_abonnements'], label='Train', linewidth=1.5, color='steelblue')\n",
    "ax.plot(df_test['date'], df_test['nb_abonnements'], label='Test', linewidth=1.5, color='darkorange')\n",
    "ax.axvline(df_train['date'].max(), color='red', linestyle='--', linewidth=2, label='Split')\n",
    "ax.set_title('Train/Test Split Temporel', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Nombre d\\'abonnements')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f779c",
   "metadata": {},
   "source": [
    "## Section 6: Prophet Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ PROPHET MODEL\")\n",
    "\n",
    "# Prepare data for Prophet (ds, y format)\n",
    "df_prophet_train = pd.DataFrame({\n",
    "    'ds': train_ts['date'].values,\n",
    "    'y': train_ts['nb_abonnements'].values\n",
    "})\n",
    "\n",
    "df_prophet_test = pd.DataFrame({\n",
    "    'ds': test_ts['date'].values,\n",
    "    'y': test_ts['nb_abonnements'].values\n",
    "})\n",
    "\n",
    "# Tunisian holidays for Prophet\n",
    "holidays = pd.DataFrame({\n",
    "    'holiday': ['Nouvel An', 'Ind√©pendance', 'Martyrs', 'F√™te du Travail', \n",
    "                'R√©publique', 'Femme', '√âvacuation'] * 2,\n",
    "    'ds': pd.to_datetime([\n",
    "        '2024-01-01', '2024-03-20', '2024-04-09', '2024-05-01',\n",
    "        '2024-07-25', '2024-08-13', '2024-10-15',\n",
    "        '2025-01-01', '2025-03-20', '2025-05-01',\n",
    "        '2025-07-25', '2025-08-13', '2025-10-15'\n",
    "    ])\n",
    "})\n",
    "\n",
    "# Train Prophet with default parameters\n",
    "start_time = time.time()\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    holidays=holidays,\n",
    "    interval_width=0.95\n",
    ")\n",
    "prophet_model.fit(df_prophet_train)\n",
    "prophet_time = time.time() - start_time\n",
    "\n",
    "# Make predictions on test set\n",
    "future_prophet_test = prophet_model.make_future_dataframe(periods=len(df_prophet_test))\n",
    "future_prophet_test = future_prophet_test[future_prophet_test['ds'].isin(df_prophet_test['ds'])]\n",
    "prophet_forecast = prophet_model.predict(future_prophet_test)\n",
    "\n",
    "y_pred_prophet = prophet_forecast['yhat'].values[:len(df_prophet_test)]\n",
    "prophet_metrics = calculate_metrics(df_prophet_test['y'].values, y_pred_prophet)\n",
    "prophet_metrics['Time'] = prophet_time\n",
    "\n",
    "print(f\"  MAE: {prophet_metrics['MAE']:.4f}\")\n",
    "print(f\"  RMSE: {prophet_metrics['RMSE']:.4f}\")\n",
    "print(f\"  MAPE: {prophet_metrics['MAPE']:.2f}%\")\n",
    "print(f\"  sMAPE: {prophet_metrics['sMAPE']:.2f}%\")\n",
    "print(f\"  Temps d'entra√Ænement: {prophet_time:.2f}s\")\n",
    "\n",
    "# Visualize Prophet predictions\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(df_prophet_test['ds'], df_prophet_test['y'], label='R√©el', linewidth=2, color='steelblue')\n",
    "ax.plot(df_prophet_test['ds'], y_pred_prophet, label='Pr√©dit (Prophet)', linewidth=2, color='darkorange', alpha=0.8)\n",
    "ax.fill_between(df_prophet_test['ds'], \n",
    "                 prophet_forecast[prophet_forecast['ds'].isin(df_prophet_test['ds'])]['yhat_lower'],\n",
    "                 prophet_forecast[prophet_forecast['ds'].isin(df_prophet_test['ds'])]['yhat_upper'],\n",
    "                 alpha=0.2, color='orange')\n",
    "ax.set_title('Prophet - Pr√©dictions sur Test Set', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Nombre d\\'abonnements')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005427d",
   "metadata": {},
   "source": [
    "## Section 7: SARIMA/Auto-ARIMA Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ SARIMA/AUTO-ARIMA MODEL\")\n",
    "\n",
    "# Auto-detect ARIMA parameters\n",
    "start_time = time.time()\n",
    "try:\n",
    "    auto_model = auto_arima(\n",
    "        y_train_full_ts,\n",
    "        seasonal=True,\n",
    "        m=7,  # Weekly seasonality\n",
    "        stepwise=True,\n",
    "        trace=False,\n",
    "        max_p=5, max_q=5, max_d=2,\n",
    "        max_P=2, max_Q=2, max_D=1,\n",
    "        information_criterion='aic'\n",
    "    )\n",
    "    print(f\"  Auto-ARIMA order: {auto_model.order}\")\n",
    "    print(f\"  Seasonal order: {auto_model.seasonal_order}\")\n",
    "    \n",
    "    # Fit on full training set and predict on test\n",
    "    y_pred_sarima = auto_model.predict(n_periods=len(y_test_full_ts))\n",
    "    sarima_time = time.time() - start_time\n",
    "    \n",
    "    sarima_metrics = calculate_metrics(y_test_full_ts, y_pred_sarima)\n",
    "    sarima_metrics['Time'] = sarima_time\n",
    "    \n",
    "    print(f\"  MAE: {sarima_metrics['MAE']:.4f}\")\n",
    "    print(f\"  RMSE: {sarima_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAPE: {sarima_metrics['MAPE']:.2f}%\")\n",
    "    print(f\"  sMAPE: {sarima_metrics['sMAPE']:.2f}%\")\n",
    "    print(f\"  Temps d'entra√Ænement: {sarima_time:.2f}s\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ax.plot(test_ts['date'].values, y_test_full_ts, label='R√©el', linewidth=2, color='steelblue')\n",
    "    ax.plot(test_ts['date'].values, y_pred_sarima, label='Pr√©dit (SARIMA)', linewidth=2, color='green', alpha=0.8)\n",
    "    ax.set_title('SARIMA - Pr√©dictions sur Test Set', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Nombre d\\'abonnements')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Erreur SARIMA: {e}\")\n",
    "    sarima_metrics = {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'sMAPE': np.nan, 'Time': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061e2cf",
   "metadata": {},
   "source": [
    "## Section 8: XGBoost Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f101ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ XGBOOST MODEL\")\n",
    "\n",
    "# Train XGBoost with default parameters\n",
    "start_time = time.time()\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    tree_method='hist'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train, verbose=False)\n",
    "xgb_time = time.time() - start_time\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "xgb_metrics = calculate_metrics(y_test, y_pred_xgb)\n",
    "xgb_metrics['Time'] = xgb_time\n",
    "\n",
    "print(f\"  MAE: {xgb_metrics['MAE']:.4f}\")\n",
    "print(f\"  RMSE: {xgb_metrics['RMSE']:.4f}\")\n",
    "print(f\"  MAPE: {xgb_metrics['MAPE']:.2f}%\")\n",
    "print(f\"  sMAPE: {xgb_metrics['sMAPE']:.2f}%\")\n",
    "print(f\"  Temps d'entra√Ænement: {xgb_time:.2f}s\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(range(len(feature_importance)), feature_importance['importance'].values, color='steelblue')\n",
    "ax.set_yticks(range(len(feature_importance)))\n",
    "ax.set_yticklabels(feature_importance['feature'].values)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('XGBoost - Top 15 Features Importance', fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(df_test['date'].values, y_test, label='R√©el', linewidth=2, color='steelblue')\n",
    "ax.plot(df_test['date'].values, y_pred_xgb, label='Pr√©dit (XGBoost)', linewidth=2, color='purple', alpha=0.8)\n",
    "ax.set_title('XGBoost - Pr√©dictions sur Test Set', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Nombre d\\'abonnements')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save XGBoost model for later\n",
    "xgb_model_path = Path('../outputs/models/xgboost_default.pkl')\n",
    "xgb_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(xgb_model_path, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"\\n‚úì Mod√®le XGBoost sauvegard√©: {xgb_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bd45b",
   "metadata": {},
   "source": [
    "## Section 9: LSTM Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ LSTM MODEL\")\n",
    "\n",
    "# Prepare LSTM data with sequences (30-day windows)\n",
    "def create_sequences(data, seq_length=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(y_train_full_ts.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create sequences from training data\n",
    "seq_length = 30\n",
    "X_seq_train, y_seq_train = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "# Use test data scaled with training scaler\n",
    "test_data_scaled = scaler.transform(y_test_full_ts.reshape(-1, 1)).flatten()\n",
    "X_seq_test, y_seq_test = create_sequences(test_data_scaled, seq_length)\n",
    "\n",
    "print(f\"  Donn√©es LSTM : X_train {X_seq_train.shape}, X_test {X_seq_test.shape}\")\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = keras.Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train LSTM\n",
    "start_time = time.time()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "history = lstm_model.fit(\n",
    "    X_seq_train.reshape(-1, seq_length, 1), y_seq_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "lstm_time = time.time() - start_time\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lstm_scaled = lstm_model.predict(X_seq_test.reshape(-1, seq_length, 1), verbose=0)\n",
    "y_pred_lstm = scaler.inverse_transform(y_pred_lstm_scaled).flatten()\n",
    "\n",
    "# Calculate metrics (only on available predictions)\n",
    "min_len = min(len(y_test_full_ts), len(y_pred_lstm))\n",
    "lstm_metrics = calculate_metrics(y_test_full_ts[:min_len], y_pred_lstm[:min_len])\n",
    "lstm_metrics['Time'] = lstm_time\n",
    "\n",
    "print(f\"  MAE: {lstm_metrics['MAE']:.4f}\")\n",
    "print(f\"  RMSE: {lstm_metrics['RMSE']:.4f}\")\n",
    "print(f\"  MAPE: {lstm_metrics['MAPE']:.2f}%\")\n",
    "print(f\"  sMAPE: {lstm_metrics['sMAPE']:.2f}%\")\n",
    "print(f\"  Temps d'entra√Ænement: {lstm_time:.2f}s\")\n",
    "\n",
    "# Visualize LSTM training history\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax.set_title('LSTM - Training History', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize LSTM predictions\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "# Adjust test dates to match predictions length\n",
    "test_dates_adjusted = test_ts['date'].values[:len(y_pred_lstm)]\n",
    "test_values_adjusted = y_test_full_ts[:len(y_pred_lstm)]\n",
    "ax.plot(test_dates_adjusted, test_values_adjusted, label='R√©el', linewidth=2, color='steelblue')\n",
    "ax.plot(test_dates_adjusted, y_pred_lstm, label='Pr√©dit (LSTM)', linewidth=2, color='red', alpha=0.8)\n",
    "ax.set_title('LSTM - Pr√©dictions sur Test Set', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Nombre d\\'abonnements')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model_path = Path('../outputs/models/lstm_default.h5')\n",
    "lstm_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "lstm_model.save(lstm_model_path)\n",
    "print(f\"\\n‚úì Mod√®le LSTM sauvegard√©: {lstm_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91825ec6",
   "metadata": {},
   "source": [
    "## Section 10: Exponential Smoothing Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ EXPONENTIAL SMOOTHING MODEL\")\n",
    "\n",
    "# Exponential Smoothing Variants\n",
    "models = {\n",
    "    'SES': ExponentialSmoothing(y_train_full_ts, trend=None, seasonal=None),\n",
    "    'Holt': ExponentialSmoothing(y_train_full_ts, trend='add', seasonal=None),\n",
    "    'Holt-Winters': ExponentialSmoothing(y_train_full_ts, trend='add', seasonal='add', seasonal_periods=7)\n",
    "}\n",
    "\n",
    "best_es_model = None\n",
    "best_es_metrics = None\n",
    "best_es_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        fitted = model.fit(optimized=True)\n",
    "        y_pred_es = fitted.forecast(len(y_test_full_ts))\n",
    "        es_time = time.time() - start_time\n",
    "        \n",
    "        metrics = calculate_metrics(y_test_full_ts, y_pred_es)\n",
    "        metrics['Time'] = es_time\n",
    "        \n",
    "        print(f\"\\n  {name}:\")\n",
    "        print(f\"    MAE: {metrics['MAE']:.4f}\")\n",
    "        print(f\"    RMSE: {metrics['RMSE']:.4f}\")\n",
    "        print(f\"    MAPE: {metrics['MAPE']:.2f}%\")\n",
    "        print(f\"    sMAPE: {metrics['sMAPE']:.2f}%\")\n",
    "        print(f\"    Temps: {es_time:.2f}s\")\n",
    "        \n",
    "        # Keep best model\n",
    "        if best_es_metrics is None or metrics['MAPE'] < best_es_metrics['MAPE']:\n",
    "            best_es_metrics = metrics\n",
    "            best_es_model = fitted\n",
    "            best_es_name = name\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Erreur {name}: {e}\")\n",
    "\n",
    "if best_es_model:\n",
    "    print(f\"\\n‚úì Meilleur mod√®le Exponential Smoothing : {best_es_name}\")\n",
    "\n",
    "# Visualize best ES model\n",
    "if best_es_model:\n",
    "    y_pred_best_es = best_es_model.forecast(len(y_test_full_ts))\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    ax.plot(test_ts['date'].values, y_test_full_ts, label='R√©el', linewidth=2, color='steelblue')\n",
    "    ax.plot(test_ts['date'].values, y_pred_best_es, label=f'Pr√©dit ({best_es_name})', linewidth=2, color='green', alpha=0.8)\n",
    "    ax.set_title(f'Exponential Smoothing ({best_es_name}) - Pr√©dictions', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Nombre d\\'abonnements')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf908d",
   "metadata": {},
   "source": [
    "## Section 11: Hyperparameter Tuning for Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98310c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è HYPERPARAMETER TUNING\")\n",
    "\n",
    "# Prophet tuning (small grid for demo)\n",
    "prophet_param_grid = {\n",
    "    'changepoint_prior_scale': [0.01, 0.05, 0.1],\n",
    "    'seasonality_prior_scale': [0.1, 1, 10],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "best_prophet_score = float('inf')\n",
    "best_prophet_params = None\n",
    "\n",
    "for cps in prophet_param_grid['changepoint_prior_scale']:\n",
    "    for sps in prophet_param_grid['seasonality_prior_scale']:\n",
    "        for mode in prophet_param_grid['seasonality_mode']:\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=False,\n",
    "                holidays=holidays,\n",
    "                changepoint_prior_scale=cps,\n",
    "                seasonality_prior_scale=sps,\n",
    "                seasonality_mode=mode\n",
    "            )\n",
    "            model.fit(df_prophet_train)\n",
    "            future = model.make_future_dataframe(periods=len(df_prophet_test))\n",
    "            future = future[future['ds'].isin(df_prophet_test['ds'])]\n",
    "            forecast = model.predict(future)\n",
    "            y_pred = forecast['yhat'].values[:len(df_prophet_test)]\n",
    "            score = mape(df_prophet_test['y'].values, y_pred)\n",
    "            if score < best_prophet_score:\n",
    "                best_prophet_score = score\n",
    "                best_prophet_params = (cps, sps, mode)\n",
    "\n",
    "print(f\"‚úÖ Best Prophet params: {best_prophet_params}, MAPE: {best_prophet_score:.2f}%\")\n",
    "\n",
    "# XGBoost tuning (small grid)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "best_xgb_score = float('inf')\n",
    "best_xgb_params = None\n",
    "\n",
    "for n_estimators in xgb_param_grid['n_estimators']:\n",
    "    for max_depth in xgb_param_grid['max_depth']:\n",
    "        for lr in xgb_param_grid['learning_rate']:\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=lr,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "                tree_method='hist'\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = mape(y_test, y_pred)\n",
    "            if score < best_xgb_score:\n",
    "                best_xgb_score = score\n",
    "                best_xgb_params = (n_estimators, max_depth, lr)\n",
    "\n",
    "print(f\"‚úÖ Best XGBoost params: {best_xgb_params}, MAPE: {best_xgb_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a30509",
   "metadata": {},
   "source": [
    "## Section 12: Model Comparison & Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c379b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics\n",
    "comparison_data = {\n",
    "    'Prophet': prophet_metrics,\n",
    "    'SARIMA': sarima_metrics,\n",
    "    'XGBoost': xgb_metrics,\n",
    "    'LSTM': lstm_metrics,\n",
    "    'Exp Smoothing': best_es_metrics\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "comparison_df = comparison_df[['MAE', 'RMSE', 'MAPE', 'sMAPE', 'Time']]\n",
    "\n",
    "print(\"üìä Model Comparison Table\")\n",
    "comparison_df.style.format('{:.3f}')\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "metrics = ['MAE', 'RMSE', 'MAPE', 'sMAPE']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    comparison_df[metric].plot(kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_title(f'Comparison by {metric}')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df['MAPE'].idxmin()\n",
    "print(f\"\\nüèÜ Best Model (MAPE): {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6b321",
   "metadata": {},
   "source": [
    "## Section 13: Final Forecasts with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà FINAL FORECASTS\")\n",
    "\n",
    "# Use best model for final forecast\n",
    "if best_model_name == 'Prophet':\n",
    "    final_model = prophet_model\n",
    "    future = final_model.make_future_dataframe(periods=90)\n",
    "    forecast = final_model.predict(future)\n",
    "    forecast_final = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(90)\n",
    "    \n",
    "elif best_model_name == 'SARIMA':\n",
    "    final_model = auto_model\n",
    "    forecast_values = final_model.predict(n_periods=90)\n",
    "    dates = pd.date_range(df_daily['date'].max() + pd.Timedelta(days=1), periods=90)\n",
    "    forecast_final = pd.DataFrame({'ds': dates, 'yhat': forecast_values})\n",
    "\n",
    "elif best_model_name == 'XGBoost':\n",
    "    final_model = xgb_model\n",
    "    last_date = df_features_clean['date'].max()\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=90)\n",
    "    \n",
    "    # Generate future features\n",
    "    future_df = pd.DataFrame({'date': future_dates})\n",
    "    future_df['jour_semaine'] = future_df['date'].dt.dayofweek\n",
    "    future_df['jour_mois'] = future_df['date'].dt.day\n",
    "    future_df['mois'] = future_df['date'].dt.month\n",
    "    future_df['trimestre'] = future_df['date'].dt.quarter\n",
    "    future_df['semaine_annee'] = future_df['date'].dt.isocalendar().week\n",
    "    future_df['est_weekend'] = future_df['jour_semaine'].isin([5, 6]).astype(int)\n",
    "    future_df['jour_annee'] = future_df['date'].dt.dayofyear\n",
    "    future_df['jour_annee_sin'] = np.sin(2 * np.pi * future_df['jour_annee'] / 365.25)\n",
    "    future_df['jour_annee_cos'] = np.cos(2 * np.pi * future_df['jour_annee'] / 365.25)\n",
    "    \n",
    "    # For lag/rolling features use last known values\n",
    "    last_data = df_features_clean.tail(30)\n",
    "    for lag in [1, 7, 14, 30]:\n",
    "        future_df[f'lag_{lag}'] = last_data['nb_abonnements'].iloc[-lag]\n",
    "    for window in [7, 14, 30]:\n",
    "        future_df[f'rolling_mean_{window}'] = last_data['nb_abonnements'].rolling(window=window).mean().iloc[-1]\n",
    "        future_df[f'rolling_std_{window}'] = last_data['nb_abonnements'].rolling(window=window).std().iloc[-1]\n",
    "    \n",
    "    # Holiday features\n",
    "    future_df['est_ferie'] = future_df['date'].isin(tunisian_holidays).astype(int)\n",
    "    \n",
    "    # Placeholder for gov/offre (assume mean)\n",
    "    for gov in top_govs:\n",
    "        future_df[f'gov_{gov}'] = df_features_clean[f'gov_{gov}'].mean()\n",
    "    for offre in top_offres:\n",
    "        future_df[f'offre_{offre}'] = df_features_clean[f'offre_{offre}'].mean()\n",
    "\n",
    "    # Predict\n",
    "    X_future = future_df[feature_cols].values\n",
    "    forecast_values = final_model.predict(X_future)\n",
    "    forecast_final = pd.DataFrame({'ds': future_df['date'], 'yhat': forecast_values})\n",
    "\n",
    "else:\n",
    "    # Default fallback: Exponential Smoothing\n",
    "    forecast_values = best_es_model.forecast(90)\n",
    "    dates = pd.date_range(df_daily['date'].max() + pd.Timedelta(days=1), periods=90)\n",
    "    forecast_final = pd.DataFrame({'ds': dates, 'yhat': forecast_values})\n",
    "\n",
    "# Export forecasts\n",
    "forecast_30 = forecast_final.head(30)\n",
    "forecast_90 = forecast_final.head(90)\n",
    "\n",
    "forecast_30.to_csv('../outputs/forecasts/forecast_30d.csv', index=False)\n",
    "forecast_90.to_csv('../outputs/forecasts/forecast_90d.csv', index=False)\n",
    "\n",
    "print(\"‚úì Pr√©visions sauvegard√©es: forecast_30d.csv, forecast_90d.csv\")\n",
    "\n",
    "# Plot final forecast\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df_daily['date'], df_daily['nb_abonnements'], label='Historique', color='gray', linewidth=1.5)\n",
    "plt.plot(forecast_final['ds'], forecast_final['yhat'], label='Pr√©visions', color='red', linewidth=2)\n",
    "plt.fill_between(forecast_final['ds'], \n",
    "                 forecast_final.get('yhat_lower', forecast_final['yhat']),\n",
    "                 forecast_final.get('yhat_upper', forecast_final['yhat']),\n",
    "                 color='red', alpha=0.2)\n",
    "plt.title('Pr√©visions 90 jours - Meilleur Mod√®le', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Nombre d\\'abonnements')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe24bf",
   "metadata": {},
   "source": [
    "## Section 14: Visualization & Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report in Markdown format\n",
    "report_path = Path('../outputs/reports/model_comparison_report.md')\n",
    "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report_content = \"# Model Comparison Report\\n\\n\"\n",
    "report_content += \"## Summary\\n\\n\"\n",
    "report_content += f\"Best Model: **{best_model_name}** (based on lowest MAPE)\\n\\n\"\n",
    "report_content += \"## Metrics Table\\n\\n\"\n",
    "report_content += comparison_df.to_markdown() + \"\\n\\n\"\n",
    "report_content += \"## Insights\\n\\n\"\n",
    "report_content += \"- Prophet: Handles seasonality and holiday effects well but may be slower.\\n\"\n",
    "report_content += \"- SARIMA: Strong baseline for seasonal data, interpretable.\\n\"\n",
    "report_content += \"- XGBoost: Captures complex feature interactions, often high accuracy.\\n\"\n",
    "report_content += \"- LSTM: Can model long dependencies but needs more data and tuning.\\n\"\n",
    "report_content += \"- Exp Smoothing: Simple and fast, best for stable trends.\\n\\n\"\n",
    "report_content += \"## Recommendations\\n\\n\"\n",
    "report_content += \"- Use the best model above for production forecasts.\\n\"\n",
    "report_content += \"- Re-train monthly as new data arrives.\\n\"\n",
    "report_content += \"- Monitor MAPE drift and retrain if MAPE > 25%.\\n\"\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"‚úì Rapport g√©n√©r√© : {report_path}\")\n",
    "print(\"\\n--- REPORT PREVIEW ---\\n\")\n",
    "print(report_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632da5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ EXPONENTIAL SMOOTHING MODEL\")\n",
    "\n",
    "# Try different exponential smoothing variants\n",
    "es_models = {}\n",
    "es_metrics_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Holt-Winters (with seasonal component)\n",
    "try:\n",
    "    hw_model = ExponentialSmoothing(\n",
    "        y_train_full_ts,\n",
    "        seasonal_periods=7,\n",
    "        trend='add',\n",
    "        seasonal='add',\n",
    "        initialization_method='estimated'\n",
    "    )\n",
    "    hw_fit = hw_model.fit(optimized=True)\n",
    "    y_pred_hw = hw_fit.forecast(steps=len(y_test_full_ts))\n",
    "    es_metrics_hw = calculate_metrics(y_test_full_ts, y_pred_hw)\n",
    "    es_metrics_hw['Method'] = 'Holt-Winters'\n",
    "    es_metrics_list.append(es_metrics_hw)\n",
    "    es_models['Holt-Winters'] = hw_fit\n",
    "    print(f\"  Holt-Winters MAPE: {es_metrics_hw['MAPE']:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Holt-Winters failed: {e}\")\n",
    "\n",
    "# Holt (linear trend, no seasonal)\n",
    "try:\n",
    "    from statsmodels.tsa.holtwinters import Holt\n",
    "    holt_model = Holt(y_train_full_ts)\n",
    "    holt_fit = holt_model.fit(optimized=True)\n",
    "    y_pred_holt = holt_fit.forecast(steps=len(y_test_full_ts))\n",
    "    es_metrics_holt = calculate_metrics(y_test_full_ts, y_pred_holt)\n",
    "    es_metrics_holt['Method'] = 'Holt'\n",
    "    es_metrics_list.append(es_metrics_holt)\n",
    "    es_models['Holt'] = holt_fit\n",
    "    print(f\"  Holt MAPE: {es_metrics_holt['MAPE']:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Holt failed: {e}\")\n",
    "\n",
    "# Simple Exponential Smoothing\n",
    "try:\n",
    "    from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "    ses_model = SimpleExpSmoothing(y_train_full_ts)\n",
    "    ses_fit = ses_model.fit(optimized=True)\n",
    "    y_pred_ses = ses_fit.forecast(steps=len(y_test_full_ts))\n",
    "    es_metrics_ses = calculate_metrics(y_test_full_ts, y_pred_ses)\n",
    "    es_metrics_ses['Method'] = 'Simple ES'\n",
    "    es_metrics_list.append(es_metrics_ses)\n",
    "    es_models['Simple ES'] = ses_fit\n",
    "    print(f\"  Simple ES MAPE: {es_metrics_ses['MAPE']:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Simple ES failed: {e}\")\n",
    "\n",
    "es_time = time.time() - start_time\n",
    "\n",
    "# Select best exponential smoothing model\n",
    "best_es_idx = np.argmin([m['MAPE'] for m in es_metrics_list])\n",
    "best_es_metrics = es_metrics_list[best_es_idx]\n",
    "best_es_metrics['Time'] = es_time\n",
    "best_es_method = es_metrics_list[best_es_idx]['Method']\n",
    "\n",
    "print(f\"\\n  Best: {best_es_method}\")\n",
    "print(f\"  MAE: {best_es_metrics['MAE']:.4f}\")\n",
    "print(f\"  RMSE: {best_es_metrics['RMSE']:.4f}\")\n",
    "print(f\"  MAPE: {best_es_metrics['MAPE']:.2f}%\")\n",
    "print(f\"  sMAPE: {best_es_metrics['sMAPE']:.2f}%\")\n",
    "print(f\"  Temps d'entra√Ænement: {es_time:.2f}s\")\n",
    "\n",
    "# Visualize best exponential smoothing predictions\n",
    "best_model = es_models[best_es_method]\n",
    "y_pred_es_best = best_model.forecast(steps=len(y_test_full_ts))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(test_ts['date'].values, y_test_full_ts, label='R√©el', linewidth=2, color='steelblue')\n",
    "ax.plot(test_ts['date'].values, y_pred_es_best, label=f'Pr√©dit ({best_es_method})', linewidth=2, color='brown', alpha=0.8)\n",
    "ax.set_title(f'Exponential Smoothing ({best_es_method}) - Pr√©dictions sur Test Set', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Nombre d\\'abonnements')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
