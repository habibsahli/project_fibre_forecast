# ğŸ§  LSTM Hyperparameter Tuning - Complete Explanation

## Overview

LSTM (Long Short-Term Memory) networks are powerful deep learning models for time-series forecasting. However, they have many hyperparameters that significantly affect performance. This guide explains:

1. **What each hyperparameter does**
2. **Why we're testing specific values**
3. **The tuning strategy**
4. **How to interpret results**

---

## ğŸ“Š Hyperparameter Explanations

### 1. **Sequence Length (seq_length)** [7, 14, 21, 30]

**What it is**: Number of past days the LSTM "looks back" to predict the next day

```
seq_length = 7
Today = Day 8
â†‘
Uses Days 1,2,3,4,5,6,7

seq_length = 30  
Today = Day 31
â†‘
Uses Days 1-30
```

**Why test different values**:
- **Too short (7 days)**: Models misses long-term patterns, prone to noise
  - âŒ Can't catch monthly trends or seasonal cycles
  - âœ… Faster training, less computation

- **Medium (14 days)**: Captures weekly patterns (good for fibre subscriptions!)
  - âœ… Weekly seasonality is natural in telecom data
  - âœ… Balanced training time

- **Longer (21-30 days)**: Captures 3-4 weeks of history
  - âœ… More context, better for complex patterns
  - âŒ Slower training, more parameters to learn

**Intuition for Fibre Data**: Telecom subscriptions have weekly patterns (business hours vs weekends), so 14-21 days is likely optimal.

---

### 2. **Learning Rate (lr)** [0.0001, 0.001, 0.01]

**What it is**: How aggressively the neural network updates its weights during training

```
Loss Function (Cost)
        â†“
        â†“ Learning Rate = 0.0001 (small steps - slow)
        â†“ 
    â•± â•² â•± â•² â•± â•²      Takes many updates to reach bottom
   â•±   â•²â•±   â•²â•± â•²
  â•±    Minimum

        â†“
        â†“ Learning Rate = 0.001 (medium steps - balanced)
        â†“
    â•±   â•²â•±        Reaches bottom efficiently
   â•±     â•²
  â•±      Minimum

        â†“
        â†“ Learning Rate = 0.01 (large steps - risky)
        â†“
    â•±     â•² â•±     Might overshoot or diverge!
   â•±       âœ—       
  â•±
```

**Why test different values**:
- **Too small (0.0001)**: 
  - âœ… More stable, less risk of diverging
  - âŒ Takes forever to train, might not converge
  
- **Medium (0.001)**: **Default for Adam optimizer**
  - âœ… Good for most cases
  - âœ… Balanced convergence speed
  
- **Larger (0.01)**:
  - âœ… Faster training
  - âŒ Risk of overshooting optimal weights, unstable convergence

**For Time-Series**: Usually 0.001 or 0.0005 works best. We test to find the sweet spot.

---

### 3. **Batch Size** [8, 16, 32]

**What it is**: Number of training samples processed before updating weights

```
Batch Size = 8:
Update 1 (8 samples)  â†’  Update weights
Update 2 (8 samples)  â†’  Update weights
Update 3 (8 samples)  â†’  Update weights
(16 updates per epoch)

Batch Size = 16:
Update 1 (16 samples)  â†’  Update weights
Update 2 (16 samples)  â†’  Update weights
(8 updates per epoch)

Batch Size = 32:
Update 1 (32 samples)  â†’  Update weights
Update 2 (32 samples)  â†’  Update weights
(4 updates per epoch)
```

**Why test different values**:
- **Small (8)**:
  - âœ… More frequent weight updates = more learning
  - âœ… Better for noisy data
  - âŒ Noisier gradients, can be unstable
  - âŒ Slower overall (more iterations)
  
- **Medium (16)**: **Good balance**
  - âœ… Moderate noise, stable convergence
  - âœ… Fast enough training
  
- **Large (32)**:
  - âœ… Smooth gradient estimates
  - âœ… Faster (fewer iterations)
  - âŒ Might miss nuances in data
  - âŒ Get stuck in local minima

**For Fibre Data**: Small batch (8-16) usually better for capturing daily variations.

---

### 4. **Dropout Rate** [0.1, 0.2, 0.3]

**What it is**: Randomly disable a percentage of neurons during training to prevent overfitting

```
Normal LSTM Layer:
[o] â†’ [o] â†’ [o] â†’ [o] â†’ [o]

Dropout = 0.1 (10% disabled):
[o] â†’ [âœ—] â†’ [o] â†’ [o] â†’ [o]  (randomly ~10% are "dropped out")

Dropout = 0.2 (20% disabled):
[o] â†’ [âœ—] â†’ [o] â†’ [âœ—] â†’ [o]  (randomly ~20% are "dropped out")

Dropout = 0.3 (30% disabled):
[âœ—] â†’ [o] â†’ [âœ—] â†’ [o] â†’ [âœ—]  (randomly ~30% are "dropped out")
```

**Why test different values**:
- **Too low (0.1)**:
  - âŒ Less regularization, higher risk of overfitting
  - âœ… Model can learn complex patterns
  
- **Medium (0.2)**: **Common default**
  - âœ… Balances generalization and learning
  - âœ… Prevents overfitting without losing capacity
  
- **Too high (0.3)**:
  - âœ… Strong regularization, less overfitting
  - âŒ Model underfits, can't learn complex patterns

**For Fibre Data**: 0.15-0.25 usually optimal. We test to find exact sweet spot.

---

### 5. **LSTM Units (Layer 1 & 2)**

**What it is**: Number of internal "memory cells" in each LSTM layer

```
Current Architecture:
Input â†’ [LSTM: 64 units] â†’ [LSTM: 32 units] â†’ Output

Each LSTM unit = a small neural network with memory

64 units = 64 parallel "learners"
32 units = 32 parallel "learners" (fewer, more compressed)
```

**Why test different values**:
- **Fewer units (32, 16)**:
  - âœ… Faster training
  - âœ… Less overfitting risk
  - âŒ Less capacity to learn complex patterns
  
- **More units (64, 128)**:
  - âœ… Greater model capacity
  - âœ… Can capture complex temporal patterns
  - âŒ Slower training
  - âŒ Higher overfitting risk
  - âŒ Need more data

**For Fibre Data**: 64 â†’ 32 (current) is good. We test if 32â†’16 (smaller) or 128â†’64 (larger) is better.

---

### 6. **Dense Layer Units** [8, 16, 32]

**What it is**: Number of neurons in the final fully-connected layer before output

```
LSTM Output â†’ [Dense Layer: 16 units] â†’ [1 unit] â†’ Prediction

Fewer (8):     Faster, less capacity
Medium (16):   Good balance
More (32):     Slower, more capacity
```

**Why test different values**:
- Small dense layer forces efficient representation
- Large dense layer can extract more patterns
- Usually 16 is optimal for fibre data

---

## ğŸ¯ Tuning Strategy Used

I'm using a **Strategic Grid Search** (not full brute-force) to keep computation time reasonable:

### Strategy:
1. **Fixed baseline**: Test defaults first
2. **One-at-a-time variation**: Change one hyperparameter, keep others fixed
3. **Independent evaluation**: Understand how each affects accuracy
4. **Combined optimization**: Blend best findings into final config

### Why this approach?
- âœ… **Feasible**: ~15 configs instead of 1,000+
- âœ… **Interpretable**: See which params matter most
- âœ… **Efficient**: Find good solutions quickly
- âš ï¸ **May miss interactions**: Some params work better together

---

## ğŸ“ˆ What We're Optimizing For

### Primary Metric: **MAPE** (Mean Absolute Percentage Error)
- Lower is better (goal: < 10% for time-series)
- Measures accuracy as a percentage

### Secondary Metrics:
- **MAE**: Average prediction error in actual units
- **RMSE**: Penalizes large errors more
- **Training Time**: Computational efficiency

### Trade-offs:
```
Accuracy vs Speed:
- More epochs â†’ Better accuracy but slower
- Larger batch â†’ Faster but less learning opportunity
- More units â†’ Better accuracy but slower
```

---

## ğŸš€ How to Run the Tuning

```bash
cd /home/habib/fibre_data_project/projet-fibre-forecast

# Run the tuning script (will test 15 different configurations)
python tune_lstm_hyperparams.py
```

**Expected output**:
- Tests each configuration with timing
- Displays top 5 best configurations
- Shows best hyperparameters
- Calculates improvement over baseline
- Saves JSON and CSV results to `outputs/lstm_tuning/`

---

## ğŸ“Š How to Interpret Results

### Example Output:
```
ğŸ¥‡ BEST CONFIGURATION

Configuration Name: Seq Length: 14 days

Hyperparameters:
   â€¢ Sequence Length:     14 days
   â€¢ LSTM Layer 1 Units:  64
   â€¢ LSTM Layer 2 Units:  32
   â€¢ Dropout Rate:        0.2
   â€¢ Batch Size:          16
   â€¢ Learning Rate:       0.001

Results:
   â€¢ MAPE:   8.45%          â† Lower than baseline (12.75%) = Better!
   â€¢ MAE:    152.34
   â€¢ RMSE:   189.23
   â€¢ Training Time: 15.42s

ğŸ“ˆ IMPROVEMENT vs BASELINE:
   Baseline MAPE: 12.75%
   Best MAPE:     8.45%
   Improvement:   âœ… 33.7% better
```

### Interpreting the Numbers:
- **MAPE 8.45%**: On average, predictions are off by 8.45% of actual values
- **33.7% better**: This config is 33.7% more accurate than the original
- **Training Time**: How long it took to train this specific configuration

---

## ğŸ” Common Patterns to Watch For

### Sign of Overfitting:
- Training loss decreases, but test MAPE increases
- Solution: Increase dropout or reduce complexity

### Sign of Underfitting:
- Both training and test losses high
- Solution: Increase model capacity (more units) or train longer

### Good Fit:
- Training loss decreases smoothly
- Test MAPE is low and stable

---

## Next Steps After Tuning

1. **If improvement > 20%**: Use new hyperparameters â†’ create new run_lstm_model.py
2. **If improvement 5-20%**: Conditional - use if deployment priority is accuracy
3. **If improvement < 5%**: Stick with baseline (simpler to maintain)
4. **If TensorFlow still unavailable**: Skip LSTM, use other 4 models

---

## Key Takeaway

Architecture choices matter, but time-series forecasting is about:
1. **Right algorithm choice** (SARIMA beats LSTM for fibre data typically)
2. **Quality of features/data** (more important than hyperparameters)
3. **Proper train/test split** (temporal ordering matters!)
4. **Domain knowledge** (weekly patterns in telecom)

LSTM is powerful but needs careful tuning and data. For your fibre dataset, SARIMA (5.38% MAPE) might still outperform tuned LSTM unless we have much more data.

